\def\year{2016}\relax
%File: formatting-instruction.tex
\documentclass[letterpaper]{article}
\usepackage{aaai16}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\frenchspacing \setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}

%\pdfinfo{Title (Data-and-Model Driven Classical Planning)/Author (Roni Stern, Brendan Juba)}


\setcounter{secnumdepth}{0}



\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{color}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{psfig}
\usepackage{subcaption}
\usepackage{tabularx}

\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\newtheorem{definition}{Definition}

%\nocopyright

\newcommand{\comment}[1]{}

\newcommand{\OPEN} {{\textsc{Open}}}

% This is for the agmin
\newcommand{\argmin}{\operatornamewithlimits{argmin}}


\newcommand{\ch}[1]
{{\color{red} #1}}

\newcommand{\qed}{\hfill\ensuremath{\blacksquare}}
\newcommand{\astar}{A$^*$}
\newcommand{\wastar}{WA$^*$}
\newcommand{\arastar}{ARA$^*$}
\newcommand{\pts}{PS}
\newcommand{\dps}{DPS}
\newcommand{\ees}{EES}
\newcommand{\gbfs}{GBFS}
\newcommand{\bees}{BEES}
\newcommand{\beeps}{BEEPS}
\newcommand{\bss}{BSS}
\newcommand{\bssb}{BSS$(B)$}
\newcommand{\bcs}{BCS}
\newcommand{\bcsb}{BCS$(C)$}
\newcommand{\ar}{AR}
\newcommand{\nr}{NR}
\newcommand{\icl}{ICL}
\newcommand{\nrr}{NRR}
\newcommand{\nrrf}{NRR1}
\newcommand{\nrrs}{NRR2}
\newcommand{\open}{\textsc{Open}}
\newcommand{\closed}{\textsc{Closed}}
\newcommand{\focal}{\textsc{Focal}}
\newcommand{\focalc}{\textsc{Focal$(C)$}}
\newcommand{\focals}{Focal Search}
\newcommand{\fmin}{$f_{min}$}


\setlength{\pdfpagewidth}{8.5in} \setlength{\pdfpageheight}{11in}
%\pdfinfo{
%/Title(To Reopen or Not to Reopen)
%/Author(Submission #)}




\setcounter{secnumdepth}{2}
\newtheorem{theorem}{Theorem}

% Used for commenting
\newcommand{\commenter}[3]{$[$\uppercase{#1}#2:#3$]$  \\}
\newcommand{\ariel}[2]{\commenter{ariel}{#1}{#2}}
\newcommand{\roni}[2]{\commenter{roni}{#1}{#2}}
\newcommand{\vitali}[2]{\commenter{vitali}{#1}{#2}}

\newcommand{\MEMO}[1]
{ \fbox{
\begin{minipage}[b]{7.9 cm}
#1
\end{minipage}
} }


\begin{document}
\title{Model-Free Contingent Planning}

\author{XXX}

\maketitle

\begin{abstract}%\vspace{-0.2cm}
In this paper we explore the theoretical boundaries of 
performing contingent planning in setting where 
there is no model of the planning agent's actions. 
Instead, a set of observed trajectories are given 
and the task is to generate a plan that is guaranteed
to achieve the goal without failing. 
We propose to address this problem by learning a conservative model of the world, 
in which actions are guaranteed to be applicable, and generating a plan with this conservative model. 
This paper studies this conservative approach, answering key questions such as 
when this approach is applicable and when the possiblity of failure is unavoidable, 
when it is complete with high likelihood, 
and what can be said about the cost of the resulting plan. 
\end{abstract}


\section{Introduction}
In classical planning problems, a model of the acting agent and its relation to the relevant world is given in the form of some of planning description language, e.g., the classical STRIPS model~\cite{fikes1971strips} or PDDL~\cite{mcdermott1998pddl}. This model is used to generate a plan, e.g., that achives some given goal condition. In this paper we address the problem of generating such a plan in cases where no model of the world is given. Instead, the input to the problem includes a set of observed trajectories, e.g., a set of plans that the agent has executed succesfully in the past. 
\[\ldots\]

In this paper we introduce a theoretical framework for learning a partial classical planning model from a given set of observed trajectories, and how to plan using these observed trajectoriesin a way that will ensure the goal is achieved. 


%[[TAKEN FROM "Planning and Learning under Uncertainty" Ph.D. thesis of Sergio Jimenez Celorrio ]] In deterministic and totally observable environments this problem has been well studied. The LIVE system [92] learns operators with quantified variables while exploring the world. The EXPO system [45] refines incomplete planning operators when the monitoring of a plan execution detects a divergence between internal expectations and external observations. [98] learns PRODIGY operators from the execution traces of the resulting solutions or execution failures. The TRAIL system [6] limits its operators to an extended version of Horn clauses so that ILP can be applied. The LOPE system [41] proposed an integrated framework for learning, planning and executing actions



\section{Problem Definition}
The setting we address in this work is a STRIPS planning problem $\Pi=\langle P, A, I, G\rangle$, where $P$ is the set of predicates, $A$ is a set of actions, $I$ is the initial state, and $G$ is the goal condition. We assume here that states in the world as well as the goal condition are defined as a conjunction of predicates from $P$. 

In the planning problem we deal with in this paper, which we refer to as {\em model-free planning}, we do not know the capabilities of the action agent directly. Instead, we are given a set of {\em trajectories}. 
\begin{definition}[Trajectory]
A trajectory $T=\langle s_1, a_1, s_2, a_2, \ldots, a_{n-1}, s_n\rangle$ is an alternating sequence of states ($s_1,\ldots,s_n$) and actions ($a_1,\ldots,a_n$) that starts and ends with a state. 
\end{definition}
The model-free planning problem is defined as follows.
\begin{definition}[Model-free planning]
The model-free planning problem is defined by the tuple $\rangle P,I,G, \mathcal{T}\rangle$, 
where $P$, $I$, and $G$ are the predicates, initial state, and goal condition, and $\mathcal{T}$ is a set of trajectories. The task is to generate plan that will achieve $G$, i.e., a sequence of action that, if applied to $I$, and results in a state that satisfies $G$.
\end{definition}

\section{Learning a Conservative Model}
We were inspired by Wang's~\cite{wang1994learning,wang1995learning} approach for learning a STRIPS model from observation and interactions. They maintain for each action $a$ two sets of possible preconditions: the set of most specific ($S(a)$) and the set of most general ($G(a)$) preconditions. Under some simplifying assumptions, which we outline below, the set of most specific preconditions $S(a)$ consists a sufficient condition for executing $a$. Under this condition, any plan that uses $S(a)$ as the set of preconditions to $a$ is guaranteed to find the goal, resulting in a sound yet incomplete solution. 

The interesting questions are: how to compute $Sa(a)$, what are the assumptions that are required for $S(a)$ to indeed be sufficient precdonios, and how many trajectories are needed to provide some guarantee of completeness. We address these questions below. 


%(no negative preconditions, no noise, full observability), it holds that the real preconditions of $a$ are a superset of $G(a)$ and a subset of $S(a)$. Thus, we can solve the model-free planning problem by generating a plan that assumes the preconditions in $S(a)$ are the true preconditions. Since they are sufficient condition, we know that the plan will succeed. 


%, $S(a)$ and $G(a)$, where $S(a)$ is a set of facts that are sufficient preconditions and $G(a)$ is a set of facts that are required preconditions. They refer to these sets as the set of most specific ($S(a)$) and the set of most general ($G(a)$) preconditions. Under some simplifying assumptions (no negative preconditions, no noise, full observability), it holds that the real preconditions of $a$ are a superset of $G(a)$ and a subset of $S(a)$. Thus, we can solve the model-free planning problem by generating a plan that assumes the preconditions in $S(a)$ are the true preconditions. Since they are sufficient condition, we know that the plan will succeed. 


We propose a conservative approach to solve the model-free planning problem
that first learns a partial model from the given trajectories and then uses it to generate a plan that is guaranteed to reach the goal. For a real, unknown, planning problem $\Pi^*=\langle P,A,I,G\rangle$ we will analyze the observed trajectories to learn the following planning problem $\Pi_L=\langle P,A_L,I,G\rangle$, where $A_L$ is the learned actions. 
Ideal
We aim that $A_L$ will 


From the set of observed trajectories ($\mathcal{T}$), we extract the set of actions that were part of the observed trajectories, denoted by $A(\mathcal{T})$. Formally,
\[ A(\mathcal{T})=\{a | \exists T\in\mathcal{T} \text{~such that~} a\in T\} \]
Next, we generate for action $a\in A(\mathcal{T})$ a set of preconditions and effects, by considering the states that were before and after $a$ in the trajectories where it appears. 
Let $T(a)$ be the set of trajectories in $\mathcal{T}$ in which $a$ appeared, i.e., 
\[ T(a)=\{T | T\in \mathcal{T} \text{~and~} a\in T\} \]


We define the preconditions and effects of $a$, denoted $pre(a)$ and $eff(a)$, respectively, as follow:
\begin{itemize}
    \item {\bf Preconditions.} The intersection of predicates that were true in all the state that preceded $a$ in the given trajectories $\mathcal{T}$. 
    \item {\bf Effects.} The difference, predicates, between states immediately before $a$ and states immediately after $a$. 
\end{itemize}

The effects are, of course, the real effects of $a$. However, the preconditions are a superset of the preconditions of $a$. Thus, every plan generated with these actions is a valid plan in the real model, but we might not find a plan we can generate even though such exists. 

MAYBE TALK ABOUT COSTS (E.G., ALL POSSIBLE SOLUTIONS TO A LINEAR EQUATION, IF WE HAVE THE COSTS OF THE TRAJECTORIES)
% hich learns from the observed trajectories a partial model that is ``safe'' to use in planning.  This results in a model that can generate a subset of the plans that the original, unknown, model can generate, but the generated plan are guaranteed to be applicable. 


%\section{Model-Free Planning as Conformant Planning}
%HERE WE'LL TALK ABOUT HOW THE ABOVE COMPILES TO CONFORMANT PLANNING,SO WE CAN JUST RUN A SUITABLE PLANNING. ALSO, SOME OF THESE PLANNERS JUST COMPILE TO CLASSICAL PLANNERS, SO THAT'S EVEN BETTER. 


\section{Thoeretical Guarantees and Limitations}



How many trajectories do we need to guarantee that every plan can be generated?


\section{Related Work}
Mour{\~{a}}o et al.~\shortcite{mourao2012learning} addressed the problem of learning a STRIPS actions given a set of observed trajectories. In their setting, the objserved trajectories consists cases in which the agent tried to perform an action an failed, while in our case the observations are only successful trajectories. Also, we assume full observability while they considered partial and noisy observations of the states in the trajectories. Their approach was to use machine learning methods to predict the outcome of an action and to predict whether it is applicable. Consequently, the plan generated by using this model can fail. By contrast, we aim for a plan that is guaranteed to work. Another similarity of this paper to the paper Mour{\~{a}}o et al.~\shortcite{mourao2012learning} 

Also related is the work of Konidarid et al.~\shortcite{konidaris2014constructing} showed how to learn a STRIPS model that provides a useful high-level plan for a continuous world. 


The mentioned above work of Wang~\cite{wang1995learning,wang1994learning} is, of course, very related to our work. 
They proposed a process for learning a STRIPS model that includes learning an action model. Unlike our work, they discussed how to refine that action model by interacting with the world, including learning from failed 
execution of actions.  We require that the plan generated by the learned action {\em must} work. This is suitable for setting where plan failure is unacceptable, e.g., where failure corresponds to physical harm to the acting agent.  


Levine and DeJong~\shortcite{levine2006explanation} proposed how to learn control operators 
from background knowledge and experimentations. A similar approach is taken in many re-inforcement learning algorithms. Again, our task is different, in that we do not allow experimentation and aim for a plan that is guaranteed to be sound.

%Wang~\cite{wang1995learning,wang1994learning} proposed a method for learning planning operators by observing trajectories and interacting with the world, observing whether learned planning operator worked  and refining the learned actions accordingly. We focus on the first part -- learning from observations -- and require that the plan generated by the learned action {\em must} work. This is suitable for setting where plan failure is unacceptable, e.g., where failure corresponds to physical harm to the acting agent.  However, we were inspired by their approach. They maintain for each action $a$ two sets of possible preconditions, $S(a)$ and $G(a)$, where $S(a)$ is a set of facts that are sufficient preconditions and $G(a)$ is a set of facts that are required preconditions. They refer to these sets as the set of most specific ($S(a)$) and the set of most general ($G(a)$) preconditions. Under some simplifying assumptions (no negative preconditions, no noise, full observability), it holds that the real preconditions of $a$ are a superset of $G(a)$ and a subset of $S(a)$. Thus, we can solve the model-free planning problem by generating a plan that assumes the preconditions in $S(a)$ are the true preconditions. Since they are sufficient condition, we know that the plan will succeed. 

\newpage

\bibliography{library}
\bibliographystyle{aaai}

\end{document}
